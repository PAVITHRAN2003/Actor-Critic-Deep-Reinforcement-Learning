{
  "cells": [
    {
      "metadata": {
        "id": "cf4d8c9b006ada45"
      },
      "cell_type": "markdown",
      "source": [
        "## <center>CSE 546: Reinforcement Learning</center>\n",
        "### <center>Prof. Alina Vereshchaka</center>\n",
        "#### <center>Spring 2025</center>\n",
        "\n",
        "Welcome to the Assignment 3, Part 1: Introduction to Actor-Critic Methods! It includes the implementation of simple actor and critic networks and best practices used in modern Actor-Critic algorithms."
      ],
      "id": "cf4d8c9b006ada45"
    },
    {
      "metadata": {
        "id": "9d7a6d891e2fb312"
      },
      "cell_type": "markdown",
      "source": [
        "## Section 0: Setup and Imports"
      ],
      "id": "9d7a6d891e2fb312"
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53473293aa9daf8e",
        "outputId": "3a666f64-92d3-4279-e4b9-cd8f7905a000"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import gymnasium as gym\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import deque\n",
        "\n",
        "# Set seed for reproducibility\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)"
      ],
      "id": "53473293aa9daf8e",
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fb3c83fe910>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "execution_count": 1
    },
    {
      "metadata": {
        "id": "2a3d9c34ff222994"
      },
      "cell_type": "markdown",
      "source": [
        "## Section 1: Actor-Critic Network Architectures and Loss Computation\n",
        "\n",
        "In this section, you will explore two common architectural designs for Actor-Critic methods and implement their corresponding loss functions using dummy tensors. These architectures are:\n",
        "- A. Completely separate actor and critic networks\n",
        "- B. A shared network with two output heads\n",
        "\n",
        "Both designs are widely used in practice. Shared networks are often more efficient and generalize better, while separate networks offer more control and flexibility.\n",
        "\n",
        "---\n"
      ],
      "id": "2a3d9c34ff222994"
    },
    {
      "metadata": {
        "id": "971fa7887dd4f858"
      },
      "cell_type": "markdown",
      "source": [
        "### Task 1a â€“ Separate Actor and Critic Networks with Loss Function\n",
        "\n",
        "Define a class `SeparateActorCritic`. Your goal is to:\n",
        "- Create two completely independent neural networks: one for the actor and one for the critic.\n",
        "- The actor should output a probability distribution over discrete actions (use `nn.Softmax`).\n",
        "- The critic should output a single scalar value.\n",
        "\n",
        " Use `nn.ReLU()` as your activation function. Include at least one hidden layer of reasonable width (e.g. 64 or 128 units).\n",
        "\n",
        "```python\n",
        "# TODO: Define SeparateActorCritic class\n",
        "```\n",
        "\n",
        " Next, simulate training using dummy tensors:\n",
        "1. Generate dummy tensors for log-probabilities, returns, estimated values, and entropies.\n",
        "2. Compute the actor loss using the advantage (return - value).\n",
        "3. Compute the critic loss as mean squared error between values and returns.\n",
        "4. Use a single optimizer for both the Actor and the Critic. In this case, combine the actor and critic losses into a total loss and perform backpropagation.\n",
        "5. Use a separate optimizers for both the Actor and the Critic. In this case, keep the actor and critic losses separate and perform backpropagation.\n",
        "\n",
        "```python\n",
        "# TODO: Simulate loss computation and backpropagation\n",
        "```\n",
        "\n",
        "ðŸ”— Helpful references:\n",
        "- PyTorch Softmax: https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html\n",
        "- PyTorch MSE Loss: https://pytorch.org/docs/stable/generated/torch.nn.functional.mse_loss.html\n",
        "\n",
        "---"
      ],
      "id": "971fa7887dd4f858"
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dd6b81ed1791e4e6",
        "outputId": "b13c3b26-0e4e-4fc9-cdc7-d64b66d1b8e0"
      },
      "cell_type": "code",
      "source": [
        "# TODO: Define a class SeparateActorCritic with separate networks for actor and critic\n",
        "\n",
        "# BEGIN_YOUR_CODE\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "### Actor-Critic with separate networks\n",
        "class SeparateActorCritic(nn.Module):\n",
        "    def __init__(self, obs_dim, action_dim, hidden_size=64):\n",
        "        super(SeparateActorCritic, self).__init__()\n",
        "\n",
        "        #Actor is used for mapping state to action probabilities\n",
        "        self.actor = nn.Sequential(\n",
        "            nn.Linear(obs_dim, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, action_dim),\n",
        "            nn.Softmax(dim=-1))  #Ensuring output is a valid probability distribution over discrete actions as mentioned\n",
        "\n",
        "        #Critic for mapping state to scalar value output\n",
        "        self.critic = nn.Sequential(\n",
        "            nn.Linear(obs_dim, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        action_probs = self.actor(x)\n",
        "        value = self.critic(x) #state-value estimate from critic\n",
        "        return action_probs, value\n",
        "\n",
        "\n",
        "#Dummy training simulation with observation and action space dimensions\n",
        "#trying to mimick the cartpole env with 4 features and 2 actions\n",
        "obs_dim = 4\n",
        "action_dim = 2\n",
        "batch_size = 8\n",
        "\n",
        "#random inputs\n",
        "dummy_states = torch.randn(batch_size, obs_dim)\n",
        "model = SeparateActorCritic(obs_dim, action_dim)\n",
        "print(\"\\n Model has been created successfully with separate actor and critic networks\")\n",
        "\n",
        "#Forward pass propagation\n",
        "action_probs, values = model(dummy_states)\n",
        "print(\"\\n Actor output with action probabilities:\")\n",
        "print(action_probs)\n",
        "print(\" Actor outputs valid probabilities \", torch.allclose(action_probs.sum(dim=1), torch.ones(batch_size), atol=1e-3)) #simple verification that each rowis summing to 1\n",
        "\n",
        "print(\"\\n Critic outputs with state value estimates:\")\n",
        "print(values)\n",
        "print(\" Critic  will output a scalar per input \", values.shape == (batch_size, 1))\n",
        "\n",
        "#Conducting categorical distribution for sampling actions and caluclating log prob, entropy values\n",
        "dist = torch.distributions.Categorical(action_probs)\n",
        "sampled_actions = dist.sample()\n",
        "log_probs = dist.log_prob(sampled_actions)\n",
        "entropy = dist.entropy()\n",
        "\n",
        "print(\"\\n Sampled actions:\", sampled_actions.tolist())\n",
        "print(\" Log probabilities:\", log_probs)\n",
        "print(\" Entropy values:\", entropy)\n",
        "\n",
        "#Dummy returns for training\n",
        "dummy_returns = torch.randn(batch_size)\n",
        "estimated_returns = dummy_returns\n",
        "advantage = estimated_returns-values.squeeze() #return - value estimate\n",
        "\n",
        "\n",
        "#Loss Computation of actor critic and total\n",
        "#here we are using a single optimizer for both actor and critic\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "actor_loss = -(log_probs * advantage.detach()).mean()\n",
        "critic_loss = F.mse_loss(values.squeeze(), dummy_returns)\n",
        "entropy_bonus = 0.01*entropy.mean()  # this is used for encouraging exploration\n",
        "total_loss = actor_loss+critic_loss -entropy_bonus\n",
        "\n",
        "print(\"\\n Actor loss:\", actor_loss.item())\n",
        "print(\"\\nCritic loss:\", critic_loss.item())\n",
        "print(\"\\n Total combined loss:\", total_loss.item())\n",
        "\n",
        "#backward Pass propagtion\n",
        "optimizer.zero_grad()\n",
        "total_loss.backward()\n",
        "optimizer.step()\n",
        "\n",
        "\n",
        "# END_YOUR_CODE"
      ],
      "id": "dd6b81ed1791e4e6",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Model has been created successfully with separate actor and critic networks\n",
            "\n",
            " Actor output with action probabilities:\n",
            "tensor([[0.5249, 0.4751],\n",
            "        [0.4818, 0.5182],\n",
            "        [0.4736, 0.5264],\n",
            "        [0.5410, 0.4590],\n",
            "        [0.4904, 0.5096],\n",
            "        [0.5208, 0.4792],\n",
            "        [0.5514, 0.4486],\n",
            "        [0.4871, 0.5129]], grad_fn=<SoftmaxBackward0>)\n",
            " Actor outputs valid probabilities  True\n",
            "\n",
            " Critic outputs with state value estimates:\n",
            "tensor([[0.2498],\n",
            "        [0.0062],\n",
            "        [0.1243],\n",
            "        [0.2787],\n",
            "        [0.2806],\n",
            "        [0.0134],\n",
            "        [0.3893],\n",
            "        [0.3038]], grad_fn=<AddmmBackward0>)\n",
            " Critic  will output a scalar per input  True\n",
            "\n",
            " Sampled actions: [1, 1, 1, 1, 0, 1, 0, 0]\n",
            " Log probabilities: tensor([-0.7442, -0.6574, -0.6417, -0.7787, -0.7125, -0.7357, -0.5954, -0.7193],\n",
            "       grad_fn=<SqueezeBackward1>)\n",
            " Entropy values: tensor([0.6919, 0.6925, 0.6918, 0.6898, 0.6930, 0.6923, 0.6879, 0.6928],\n",
            "       grad_fn=<NegBackward0>)\n",
            "\n",
            " Actor loss: -0.09635810554027557\n",
            "\n",
            "Critic loss: 0.8187076449394226\n",
            "\n",
            " Total combined loss: 0.715434730052948\n"
          ]
        }
      ],
      "execution_count": 25
    },
    {
      "metadata": {
        "id": "eb8e90c88108cd2e"
      },
      "cell_type": "markdown",
      "source": [
        "### Discuss the motivation behind each setup and when it may be preferred in practice.\n",
        "\n",
        "YOUR ANSWER:\n",
        "Making use of separate actor and critic networks gives the agent more flexibility to learn policy and value functions independently because it is useful when the action selection differ from state evaluation, which happens where there might be high stochasticity. We have also learnt that allowing a separate networks help to prevent gradients from interfering across tasks and may improves stability. This setup is mainly useful whenever we use a different learning for each role, that is for actor and critic allowing to have its own feature extraction idependant of its role."
      ],
      "id": "eb8e90c88108cd2e"
    },
    {
      "metadata": {
        "id": "98ea382314354335"
      },
      "cell_type": "code",
      "source": [],
      "id": "98ea382314354335",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "64081a606b93029d"
      },
      "cell_type": "markdown",
      "source": [
        "### Task 1b â€“ Shared Network with Actor and Critic Heads + Loss Function\n",
        "\n",
        "Now define a class `SharedActorCritic`:\n",
        "- Build a shared base network (e.g., linear layer + ReLU)\n",
        "- Create two heads: one for actor (output action probabilities) and one for critic (output state value)\n",
        "\n",
        "```python\n",
        "# TODO: Define SharedActorCritic class\n",
        "```\n",
        "\n",
        "Then:\n",
        "1. Pass a dummy input tensor through the model to obtain action probabilities and value.\n",
        "2. Simulate dummy rewards and compute advantage.\n",
        "3. Compute the actor and critic losses, combine them, and backpropagate.\n",
        "\n",
        "```python\n",
        "# TODO: Simulate shared network loss computation and backpropagation\n",
        "```\n",
        "\n",
        " Use `nn.Softmax` for actor output and `nn.Linear` for scalar critic output.\n",
        "\n",
        "ðŸ”— More reading:\n",
        "- Policy Gradient Methods: https://spinningup.openai.com/en/latest/algorithms/vpg.html\n",
        "- Actor-Critic Overview: https://www.tensorflow.org/agents/tutorials/6_reinforce_tutorial\n",
        "- PyTorch Categorical Distribution: https://pytorch.org/docs/stable/distributions.html#categorical\n",
        "\n",
        "---"
      ],
      "id": "64081a606b93029d"
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a48f882fff11aecc",
        "outputId": "9623fa1a-379c-4a0a-d309-5b0f7f3aefc2"
      },
      "cell_type": "code",
      "source": [
        "# BEGIN_YOUR_CODE\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "\n",
        "#Shared Actor-Critic Class\n",
        "class SharedActorCritic(nn.Module):\n",
        "    def __init__(self, obs_dim, action_dim, hidden_size=64):\n",
        "        super(SharedActorCritic, self).__init__()\n",
        "\n",
        "        #Shared hidden layers structured\n",
        "        self.shared = nn.Sequential(\n",
        "            nn.Linear(obs_dim, hidden_size),\n",
        "            nn.ReLU() )\n",
        "        #Create two heads: one for actor and one for critic\n",
        "\n",
        "        #Actor head outputs probabilities distribution\n",
        "        self.actor_head = nn.Sequential(\n",
        "            nn.Linear(hidden_size, action_dim),\n",
        "            nn.Softmax(dim=-1))\n",
        "\n",
        "        #Critic head outputs state value - scalar\n",
        "        self.critic_head = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        shared_out = self.shared(x)\n",
        "        action_probs = self.actor_head(shared_out)\n",
        "        state_value = self.critic_head(shared_out)\n",
        "        return action_probs, state_value\n",
        "\n",
        "\n",
        "#dummy data for testing istantiation\n",
        "obs_dim = 4\n",
        "action_dim = 2\n",
        "batch_size = 8\n",
        "\n",
        "#Dummy inputs\n",
        "dummy_states = torch.randn(batch_size, obs_dim)\n",
        "\n",
        "model = SharedActorCritic(obs_dim, action_dim)\n",
        "action_probs, values = model(dummy_states)\n",
        "\n",
        "#Converts action_probs to a categorical distribution and samples actions\n",
        "dist = Categorical(action_probs)\n",
        "actions = dist.sample()\n",
        "log_probs = dist.log_prob(actions)\n",
        "entropy = dist.entropy()\n",
        "\n",
        "#Dummy rewards/returns and advantage\n",
        "returns = torch.randn(batch_size)\n",
        "advantage = returns-values.squeeze()\n",
        "\n",
        "print(\"\\n Action probabilities:\")\n",
        "print(action_probs)\n",
        "print(\"\\n Sampled actions:\", actions.tolist())\n",
        "\n",
        "print(\"\\n Log probs:\", log_probs)\n",
        "print(\" Entropy:\", entropy)\n",
        "print(\"\\n Estimated values:\", values.squeeze())\n",
        "print(\" Dummy returns:\", returns)\n",
        "print(\" Computed advantage:\", advantage)\n",
        "\n",
        "#Loss Computation of actor, critic and total\n",
        "actor_loss = -(log_probs * advantage.detach()).mean()\n",
        "critic_loss = F.mse_loss(values.squeeze(), returns)\n",
        "entropy_bonus = 0.01*entropy.mean()\n",
        "total_loss = actor_loss+critic_loss- entropy_bonus\n",
        "\n",
        "print(\"\\n Actor Loss:\", actor_loss.item())\n",
        "print(\" Critic Loss:\", critic_loss.item())\n",
        "print(\" Total Loss:\", total_loss.item())\n",
        "\n",
        "#Backward pass porpagation\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "optimizer.zero_grad()\n",
        "total_loss.backward()\n",
        "optimizer.step()\n",
        "\n",
        "# END_YOUR_CODE"
      ],
      "id": "a48f882fff11aecc",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Action probabilities:\n",
            "tensor([[0.5670, 0.4330],\n",
            "        [0.6325, 0.3675],\n",
            "        [0.5680, 0.4320],\n",
            "        [0.5590, 0.4410],\n",
            "        [0.6758, 0.3242],\n",
            "        [0.5835, 0.4165],\n",
            "        [0.5669, 0.4331],\n",
            "        [0.4981, 0.5019]], grad_fn=<SoftmaxBackward0>)\n",
            "\n",
            " Sampled actions: [0, 0, 1, 0, 0, 0, 1, 1]\n",
            "\n",
            " Log probs: tensor([-0.5674, -0.4580, -0.8393, -0.5815, -0.3918, -0.5387, -0.8368, -0.6893],\n",
            "       grad_fn=<SqueezeBackward1>)\n",
            " Entropy: tensor([0.6841, 0.6576, 0.6839, 0.6862, 0.6300, 0.6791, 0.6842, 0.6931],\n",
            "       grad_fn=<NegBackward0>)\n",
            "\n",
            " Estimated values: tensor([-0.3125, -0.4235, -0.4264, -0.1971, -0.7151, -0.3151, -0.3972, -0.3350],\n",
            "       grad_fn=<SqueezeBackward0>)\n",
            " Dummy returns: tensor([ 1.7314,  1.0503, -0.5060,  1.4258,  0.3559, -1.2954,  1.1293, -0.4279])\n",
            " Computed advantage: tensor([ 2.0440,  1.4738, -0.0795,  1.6230,  1.0710, -0.9803,  1.5265, -0.0930],\n",
            "       grad_fn=<SubBackward0>)\n",
            "\n",
            " Actor Loss: 0.47707635164260864\n",
            " Critic Loss: 1.6796519756317139\n",
            " Total Loss: 2.1499805450439453\n"
          ]
        }
      ],
      "execution_count": 26
    },
    {
      "metadata": {
        "id": "a974e302d1fdb028"
      },
      "cell_type": "markdown",
      "source": [
        "### Discuss the motivation behind each setup and when it may be preferred in practice.\n",
        "\n",
        "YOUR ANSWER:\n",
        "Making use of a shared network with two heads allows the actor and critic\n",
        " maybe computationally cheaper and could promote for generalization where there will be enviornemnts with high-dimensional states like dense images. As the  computation is efficient with shared head-network the agent is expected to learn from shared signals but may fail in noisy environments as it may interfere with policy/value learning, and separate networks are advised then.Faster learning and sharing layers - reduces parameters thereby, reducing memory and reusing features."
      ],
      "id": "a974e302d1fdb028"
    },
    {
      "metadata": {
        "id": "8fad5ea9406f8b4b"
      },
      "cell_type": "code",
      "source": [],
      "id": "8fad5ea9406f8b4b",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "eb645eb009b85b1c"
      },
      "cell_type": "markdown",
      "source": [
        "## Section 2: Auto-Adaptive Network Setup for Environments\n",
        "\n",
        "You will now create a function that builds a shared actor-critic network that adapts to any Gymnasium environment. This function should inspect the environment and build input/output layers accordingly."
      ],
      "id": "eb645eb009b85b1c"
    },
    {
      "metadata": {
        "id": "4223b6ddf43abee5"
      },
      "cell_type": "markdown",
      "source": [
        "### Task 2: Auto-generate Input and Output Layers\n",
        "Write a function `create_shared_network(env)` that constructs a neural network using the following rules:\n",
        "- The input layer should match the environment's observation space.\n",
        "- The output layer for the **actor** should depend on the action space:\n",
        "  - For discrete actions: output probabilities using `nn.Softmax`.\n",
        "  - For continuous actions: output mean and log std for a Gaussian distribution.\n",
        "- The **critic** always outputs a single scalar value.\n",
        "\n",
        "```python\n",
        "# TODO: Define function `create_shared_network(env)`\n",
        "```\n",
        "\n",
        "#### Environments to Support:\n",
        "Test your function with the following environments:\n",
        "1. `CliffWalking-v0` (Use one-hot encoding for discrete integer observations.)\n",
        "2. `LunarLander-v3` (Standard Box space for observations and discrete actions.)\n",
        "3. `PongNoFrameskip-v4` (Use gym wrappers for Atari image preprocessing.)\n",
        "4. `HalfCheetah-v5` (Continuous observation and continuous action.)\n",
        "\n",
        "```python\n",
        "# TODO: Loop through environments and test `create_shared_network`\n",
        "```\n",
        "\n",
        "Hint: Use `gym.spaces` utilities to determine observation/action types dynamically.\n",
        "\n",
        "ðŸ”— Observation/Action Space Docs:\n",
        "- https://gymnasium.farama.org/api/spaces/\n",
        "\n",
        "---"
      ],
      "id": "4223b6ddf43abee5"
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update && apt-get install -y swig"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hV3GKcI4Emjc",
        "outputId": "a33e7166-2492-40f0-e0fc-6cbef80236cd"
      },
      "id": "hV3GKcI4Emjc",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Hit:6 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Fetched 257 kB in 1s (197 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "swig is already the newest version (4.0.2-1ubuntu1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 36 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install box2d-py==2.3.5\n",
        "!python -c \"import Box2D; print('Box2D imported successfully')\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12pVlGbFEop1",
        "outputId": "c9e0e4b8-6859-4462-e379-0a804809d5b6"
      },
      "id": "12pVlGbFEop1",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: box2d-py==2.3.5 in /usr/local/lib/python3.11/dist-packages (2.3.5)\n",
            "Box2D imported successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"gymnasium[mujoco]\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMaOh54fGAXi",
        "outputId": "e44b2492-5326-4a7c-af72-72e4cb2e92e9"
      },
      "id": "MMaOh54fGAXi",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium[mujoco] in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[mujoco]) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[mujoco]) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[mujoco]) (4.13.1)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[mujoco]) (0.0.4)\n",
            "Requirement already satisfied: mujoco>=2.1.5 in /usr/local/lib/python3.11/dist-packages (from gymnasium[mujoco]) (3.3.1)\n",
            "Requirement already satisfied: imageio>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[mujoco]) (2.37.0)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.11/dist-packages (from imageio>=2.14.1->gymnasium[mujoco]) (11.1.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from mujoco>=2.1.5->gymnasium[mujoco]) (1.4.0)\n",
            "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.11/dist-packages (from mujoco>=2.1.5->gymnasium[mujoco]) (1.12.2)\n",
            "Requirement already satisfied: glfw in /usr/local/lib/python3.11/dist-packages (from mujoco>=2.1.5->gymnasium[mujoco]) (2.9.0)\n",
            "Requirement already satisfied: pyopengl in /usr/local/lib/python3.11/dist-packages (from mujoco>=2.1.5->gymnasium[mujoco]) (3.1.9)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from etils[epath]->mujoco>=2.1.5->gymnasium[mujoco]) (2025.3.2)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.11/dist-packages (from etils[epath]->mujoco>=2.1.5->gymnasium[mujoco]) (6.5.2)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.11/dist-packages (from etils[epath]->mujoco>=2.1.5->gymnasium[mujoco]) (3.21.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import ale_py\n",
        "print([env_id for env_id in gym.registry if \"Pong\" in env_id])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LvvdxYFVQLqD",
        "outputId": "a6192f68-701c-4ceb-b144-d97bc6491636"
      },
      "id": "LvvdxYFVQLqD",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Pong-v0', 'PongDeterministic-v0', 'PongNoFrameskip-v0', 'Pong-v4', 'PongDeterministic-v4', 'PongNoFrameskip-v4', 'Pong-ram-v0', 'Pong-ramDeterministic-v0', 'Pong-ramNoFrameskip-v0', 'Pong-ram-v4', 'Pong-ramDeterministic-v4', 'Pong-ramNoFrameskip-v4', 'ALE/Pong-v5', 'ALE/Pong-ram-v5']\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "d6d249ff9277403a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8862fbb2-4939-4644-d844-f42f7cc6c334"
      },
      "cell_type": "code",
      "source": [
        "###Shared Actor-Critic Network (Auto-Adaptive)\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "from gymnasium.wrappers import AtariPreprocessing\n",
        "import ale_py\n",
        "\n",
        "\n",
        "#Define Auto-Adaptive Actor-Critic Network\n",
        "class AutoSharedActorCritic(nn.Module):\n",
        "    def __init__(self, input_dim, action_space): #2-layer MLP mapping input to shared features\n",
        "        super().__init__()\n",
        "        self.shared = nn.Sequential(\n",
        "            nn.Linear(input_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU())\n",
        "\n",
        "        if isinstance(action_space, gym.spaces.Discrete):\n",
        "            self.actor_head = nn.Sequential(\n",
        "                nn.Linear(64, action_space.n),\n",
        "                nn.Softmax(dim=-1))\n",
        "            self.is_discrete = True\n",
        "\n",
        "        elif isinstance(action_space, gym.spaces.Box):\n",
        "            self.actor_mean = nn.Linear(64, action_space.shape[0])\n",
        "            self.actor_logstd = nn.Parameter(torch.zeros(action_space.shape[0]))\n",
        "            self.is_discrete = False\n",
        "\n",
        "        self.critic_head = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, x): #applies shared and then to estimate V(s) - critic head\n",
        "        shared_out = self.shared(x)\n",
        "        value = self.critic_head(shared_out)\n",
        "        if self.is_discrete:\n",
        "            action_probs = self.actor_head(shared_out) #output acrtion prob\n",
        "            return action_probs, value\n",
        "        else:\n",
        "            mean = self.actor_mean(shared_out)\n",
        "            std = torch.exp(self.actor_logstd)\n",
        "            return (mean, std), value #output mean std, of gaussian distr.\n",
        "\n",
        "\n",
        "#Auto Network Constructor -builds a model based on the environment structure\n",
        "def create_shared_network(env):\n",
        "    obs_space = env.observation_space\n",
        "    if isinstance(obs_space, gym.spaces.Discrete):\n",
        "        input_dim = obs_space.n\n",
        "    elif isinstance(obs_space, gym.spaces.Box):\n",
        "        input_dim = int(np.prod(obs_space.shape))\n",
        "\n",
        "    return AutoSharedActorCritic(input_dim, env.action_space)\n",
        "\n",
        "\n",
        "def test_network_output(env, model, env_name): #prints the model outputs for a given environment\n",
        "    print(f\"\\n Currently Testing: {env_name}\")\n",
        "    obs, _ = env.reset()\n",
        "\n",
        "    if isinstance(env.observation_space, gym.spaces.Discrete):\n",
        "        obs_tensor = torch.zeros(env.observation_space.n)\n",
        "        obs_tensor[obs] = 1.0\n",
        "    else:\n",
        "        obs_tensor = torch.tensor(obs, dtype=torch.float32).flatten()\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        actor_out, critic_out = model(obs_tensor)\n",
        "\n",
        "    if isinstance(env.action_space, gym.spaces.Discrete):\n",
        "        print(\" Actor Output prob:\", np.round(actor_out.numpy(), 4))\n",
        "    else:\n",
        "        mean, std = actor_out\n",
        "        print(\" Actor Output- Mean:\", np.round(mean.numpy(), 4))\n",
        "        print(\" Actor Output- Std Dev:\", np.round(std.numpy(), 4))\n",
        "\n",
        "    print(\" Critic Output val:\", round(critic_out.item(), 4))\n",
        "\n",
        "#CliffWalking-v0\n",
        "env1 = gym.make(\"CliffWalking-v0\")\n",
        "model1 = create_shared_network(env1)\n",
        "print(\" CliffWalking-v0 created.\")\n",
        "test_network_output(env1, model1, \"CliffWalking-v0\")\n",
        "\n",
        "#LunarLander-v3\n",
        "env2 = gym.make(\"LunarLander-v3\")\n",
        "model2 = create_shared_network(env2)\n",
        "print(\"\\n LunarLander-v3 created.\")\n",
        "test_network_output(env2, model2, \"LunarLander-v3\")\n",
        "\n",
        "#PongNoFrameskip-v4 - pongv4 had installation ALE_PY and ROM issues referred from gymnasium documentation\n",
        "pong = gym.make(\"PongNoFrameskip-v4\", render_mode=\"rgb_array\", frameskip=1)\n",
        "env3 = AtariPreprocessing(pong, grayscale_obs=True, scale_obs=True)\n",
        "model3 = create_shared_network(env3)\n",
        "print(\"\\n PongNoFrameskip-v4 created and preprocessed.\")\n",
        "test_network_output(env3, model3, \"PongNoFrameskip-v4\")\n",
        "\n",
        "\n",
        "#HalfCheetah-v5\n",
        "env4 = gym.make(\"HalfCheetah-v5\")\n",
        "model4 = create_shared_network(env4)\n",
        "print(\"\\n HalfCheetah-v5 created.\")\n",
        "test_network_output(env4, model4, \"HalfCheetah-v5\")\n"
      ],
      "id": "d6d249ff9277403a",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " CliffWalking-v0 created.\n",
            "\n",
            " Currently Testing: CliffWalking-v0\n",
            " Actor Output prob: [0.2635 0.2568 0.233  0.2467]\n",
            " Critic Output val: 0.0039\n",
            "\n",
            " LunarLander-v3 created.\n",
            "\n",
            " Currently Testing: LunarLander-v3\n",
            " Actor Output prob: [0.2696 0.2486 0.2652 0.2166]\n",
            " Critic Output val: -0.0138\n",
            "\n",
            " PongNoFrameskip-v4 created and preprocessed.\n",
            "\n",
            " Currently Testing: PongNoFrameskip-v4\n",
            " Actor Output prob: [0.1776 0.1519 0.1723 0.18   0.1483 0.1699]\n",
            " Critic Output val: 0.0415\n",
            "\n",
            " HalfCheetah-v5 created.\n",
            "\n",
            " Currently Testing: HalfCheetah-v5\n",
            " Actor Output- Mean: [-0.0504 -0.0403 -0.1124  0.0041 -0.0766  0.1555]\n",
            " Actor Output- Std Dev: [1. 1. 1. 1. 1. 1.]\n",
            " Critic Output val: -0.1304\n"
          ]
        }
      ],
      "execution_count": 27
    },
    {
      "metadata": {
        "id": "4ccd13f0b62b30ff"
      },
      "cell_type": "markdown",
      "source": [
        "### Discuss the motivation behind each setup and when it may be preferred in practice.\n",
        "\n",
        "YOUR ANSWER:\n",
        "The auto-adaptive design helps detect the environment type which not only saves time with different gym enviornments with very different state/action shapes. It allows to design an actor-critic architecture to generalize with all the environments. With the autosetup for discrete action spaces we used the actor head to adapt to output action probabilities and gaussian distribution for continous as in the CliffWalking-v0 gives an integer-based observation and expects a discrete action,LunarLander-v3 gives a continuous Box vector and expects a discrete action, HalfCheetah-v5 outputs and accepts continuous vectors and finally PongNoFrameskip-v4 deals with image frames and discrete actions. If weare to use custom made architetcures for each task, this method may have to be avoided.\n"
      ],
      "id": "4ccd13f0b62b30ff"
    },
    {
      "metadata": {
        "id": "ee2dd81024ce246a"
      },
      "cell_type": "code",
      "source": [],
      "id": "ee2dd81024ce246a",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "b39c886fa536a639"
      },
      "cell_type": "markdown",
      "source": [
        "### Task 3: Write Observation Normalization Function\n",
        "Create a function `normalize_observation(obs, env)` that:\n",
        "- Checks if the observation space is `Box` and has `low` and `high` attributes.\n",
        "- If so, normalize the input observation.\n",
        "- Otherwise, return the observation unchanged.\n",
        "\n",
        "```python\n",
        "# TODO: Define `normalize_observation(obs, env)`\n",
        "```\n",
        "\n",
        "Test this function with observations from:\n",
        "- `LunarLander-v3`\n",
        "- `PongNoFrameskip-v4`\n",
        "\n",
        "Note: Atari observations are image arrays. Normalize pixel values to [0, 1]. For LunarLander-v3, the different elements in the observation vector have different ranges. Normalize them to [0, 1] using the `low` and `high` attributes of the observation space.\n",
        "\n",
        "\n",
        "---"
      ],
      "id": "b39c886fa536a639"
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fc7ee06112cf7d29",
        "outputId": "1bdc1a02-b422-4f0c-edf7-8b10bfeb5777"
      },
      "cell_type": "code",
      "source": [
        "#Task 3: Normalize Observations\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from gymnasium.wrappers import AtariPreprocessing\n",
        "import ale_py\n",
        "\n",
        "\n",
        "# Normalization Function\n",
        "def normalize_observation(obs, env):# The understanding of Normalizing observations for the environment type, box space with finite low/high is matched to [0, 1], images scale to [0,1] and discrete is same return\n",
        "    obs_space = env.observation_space\n",
        "\n",
        "    if isinstance(obs_space, gym.spaces.Box):\n",
        "        #Real-valued vector obs - Check if the observation space is continuous\n",
        "        if np.isfinite(obs_space.low).all() and np.isfinite(obs_space.high).all(): #Handles bounded Box spaces\n",
        "            norm_obs = (obs-obs_space.low)/ (obs_space.high-obs_space.low+ 1e-8) #for small vals\n",
        "            return norm_obs\n",
        "\n",
        "        #Image obs divides by 255 to normalize image observations\n",
        "        if obs.max() > 1.0:\n",
        "            return obs/ 255.0\n",
        "\n",
        "    # discrete return\n",
        "    return obs\n",
        "\n",
        "#LunarLander-v3 -Box obs\n",
        "print(\"\\n Testing LunarLander-v3\")\n",
        "env1 = gym.make(\"LunarLander-v3\")\n",
        "obs1, _ = env1.reset()\n",
        "\n",
        "print(\"Raw obs:\", obs1)\n",
        "norm1 = normalize_observation(obs1, env1)\n",
        "print(\"Normalized obs:\", norm1)\n",
        "print(\"Range:\", np.min(norm1), \"to\", np.max(norm1))\n",
        "\n",
        "#The different elements in the observation vector have different ranges - printed out\n",
        "# Test: PongNoFrameskip-v4 (Image obs)\n",
        "print(\"\\n Testing PongNoFrameskip-v4\")\n",
        "env2 = AtariPreprocessing(gym.make(\"PongNoFrameskip-v4\"), grayscale_obs=True, scale_obs=True)\n",
        "obs2, _ = env2.reset()\n",
        "print(\"Raw obs shape:\", obs2.shape)\n",
        "norm2 = normalize_observation(obs2, env2)\n",
        "print(\"Normalized shape:\", norm2.shape)\n",
        "print(\"Range:\", np.min(norm2), \"to\", np.max(norm2))\n",
        "\n"
      ],
      "id": "fc7ee06112cf7d29",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Testing LunarLander-v3\n",
            "Raw obs: [ 0.0015069   1.4077027   0.15262283 -0.14300013 -0.00173939 -0.03457131\n",
            "  0.          0.        ]\n",
            "Normalized obs: [0.50030136 0.7815405  0.5076312  0.49285    0.49986157 0.4982714\n",
            " 0.         0.        ]\n",
            "Range: 0.0 to 0.7815405\n",
            "\n",
            " Testing PongNoFrameskip-v4\n",
            "Raw obs shape: (84, 84)\n",
            "Normalized shape: (84, 84)\n",
            "Range: 0.20392157 to 0.9254902\n"
          ]
        }
      ],
      "execution_count": 31
    },
    {
      "metadata": {
        "id": "501ed2a6e7ca7a7b"
      },
      "cell_type": "markdown",
      "source": [
        "### Discuss the motivation behind each setup and when it may be preferred in practice.\n",
        "\n",
        "YOUR ANSWER:\n",
        "Normalizing fucntions in RL is important for improving the learning performance because some environments return image-based inputs as in Pong and LunarLander gives vetcors of real value which now help to make all input features lie in a similar range. Another observation for the same is the usefulness in preventing gradient explosions/vanishing gradients and help in converegnce, thus enabling consustency across varying state-representations. In this case, LunarLander and PongNoFrameskip-v4  gives real value vectors and pixel based inputs which vary in scale and can create instability while training."
      ],
      "id": "501ed2a6e7ca7a7b"
    },
    {
      "metadata": {
        "id": "78211b617a843f62"
      },
      "cell_type": "code",
      "source": [],
      "id": "78211b617a843f62",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "6b5fb5353307f514"
      },
      "cell_type": "markdown",
      "source": [
        "## Section 4: Gradient Clipping\n",
        "\n",
        "To prevent exploding gradients, it's common practice to clip gradients before optimizer updates.\n",
        "\n",
        "### Task 4: Clip Gradients for Actor-Critic Networks\n",
        "Use dummy tensors and apply gradient clipping with the following PyTorch method:\n",
        "```python\n",
        "# During training, after loss.backward():\n",
        "torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
        "```\n",
        "\n",
        "Reuse the loss computation from Task 1a or 1b. After computing the gradients, apply gradient clipping.\n",
        "Print the gradient norm before and after clipping to verify itâ€™s applied.\n",
        "\n",
        "ðŸ”— PyTorch Docs: https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html\n",
        "\n",
        "\n",
        "---"
      ],
      "id": "6b5fb5353307f514"
    },
    {
      "metadata": {
        "id": "7327507fb6e803ad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c66e52a4-89f5-4b91-e8db-ef089442b351"
      },
      "cell_type": "code",
      "source": [
        "#Task 4: Gradient Clipping with Shared Actor-Critic\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "#Shared Actor-Critic from the previous module\n",
        "class SharedActorCritic(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.shared = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU())\n",
        "        self.actor_head = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, output_dim),\n",
        "            nn.Softmax(dim=-1))\n",
        "        self.critic_head = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        base = self.shared(x)\n",
        "        action_probs = self.actor_head(base)\n",
        "        value = self.critic_head(base)\n",
        "        return action_probs, value\n",
        "\n",
        "\n",
        "#Setup with Dummy Inputs\n",
        "input_dim = 4    #Simulated observation size\n",
        "output_dim = 2   #Two possible discrete actions\n",
        "hidden_dim = 64 #Hidden layer size\n",
        "\n",
        "model = SharedActorCritic(input_dim, hidden_dim, output_dim)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
        "\n",
        "#Dummystate instantiation\n",
        "state = torch.tensor([[1.0, 0.0, 0.0, 0.0]])\n",
        "#Simulated dummy return\n",
        "dummy_return = torch.tensor([[1.5]])\n",
        "\n",
        "#Forward Pass & Loss- Passing through model\n",
        "action_probs, value = model(state)\n",
        "print(\"\\n Action Probabilities:\", action_probs.detach().numpy())\n",
        "\n",
        "#building a categorical distribution, log prob, advatnage\n",
        "dist = Categorical(probs=action_probs)\n",
        "action = dist.sample()\n",
        "print(\" Sampled Action:\", action.item())\n",
        "log_prob = dist.log_prob(action)\n",
        "print(\" Log Probability of Chosen Action:\", log_prob.item())\n",
        "advantage = dummy_return - value\n",
        "print(\" Advantage:\", advantage.item())\n",
        "\n",
        "#actor, critic and total Losses\n",
        "actor_loss = -log_prob*advantage.detach()\n",
        "critic_loss = nn.MSELoss()(value, dummy_return)\n",
        "total_loss = actor_loss+critic_loss\n",
        "print(\"\\n Actor Loss:\", actor_loss.item())\n",
        "print(\" Critic Loss:\", critic_loss.item())\n",
        "print(\" Total Loss:\", total_loss.item())\n",
        "\n",
        "#Gradient Clipping section\n",
        "optimizer.zero_grad()\n",
        "total_loss.backward()\n",
        "\n",
        "#Before clipping\n",
        "total_norm_before = torch.norm(\n",
        "    torch.stack([torch.norm(p.grad.detach()) for p in model.parameters()]))\n",
        "print(\"\\n Gradient Norm Before Clipping:\", round(total_norm_before.item(), 4))\n",
        "\n",
        "#applying gradient clipping at 0.5\n",
        "clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
        "total_norm_after = torch.norm(\n",
        "    torch.stack([torch.norm(p.grad.detach()) for p in model.parameters()]))\n",
        "print(\" Gradient Norm After Clipping:\", round(total_norm_after.item(), 4))\n",
        "\n",
        "#final Update model\n",
        "optimizer.step()\n",
        "print(\" Optimizer step completed with clipped gradients operation\")\n"
      ],
      "id": "7327507fb6e803ad",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Action Probabilities: [[0.46755075 0.53244925]]\n",
            " Sampled Action: 1\n",
            " Log Probability of Chosen Action: -0.6302676796913147\n",
            " Advantage: 1.6202468872070312\n",
            "\n",
            " Actor Loss: 1.0211892127990723\n",
            " Critic Loss: 2.625200033187866\n",
            " Total Loss: 3.6463892459869385\n",
            "\n",
            " Gradient Norm Before Clipping: 7.9327\n",
            " Gradient Norm After Clipping: 0.5\n",
            " Optimizer step completed with clipped gradients operation\n"
          ]
        }
      ],
      "execution_count": 32
    },
    {
      "metadata": {
        "id": "9952750fa74cd487"
      },
      "cell_type": "markdown",
      "source": [
        "### Discuss the motivation behind each setup and when it may be preferred in practice.\n",
        "\n",
        "YOUR ANSWER:\n",
        "When training actor-critic networks, sometimes gradients may vanish or explode when dealing with deep architectures and to avoid this we use gradient clipping that ensures the overall norm of the gradient stays below a specified max_norm like 0.5. This may help to stabilize learning and avoid overshooting updates and on the overall makes training more reliable in different environments. With respect to LunarLander or Pong, early training gives much varying results and gradient clipping acts as a stabilizer. Rewards with very high variance along with the policy gradient -- value estimation error may make the learning unstable, hence limiting it to 0.5 - the total gradient scales down and recommended for shared architectures.\n",
        "\n"
      ],
      "id": "9952750fa74cd487"
    },
    {
      "metadata": {
        "id": "557a9303f5a1c863"
      },
      "cell_type": "code",
      "source": [],
      "id": "557a9303f5a1c863",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "f4cff31e6c6e7e4a"
      },
      "cell_type": "markdown",
      "source": [
        "If you are working in a team, provide a contribution summary.\n",
        "| Team Member | Step# | Contribution (%) |  \n",
        "Team Member 1 - T1 -  Pavitran Gnanasekaran (pgnanase)  \n",
        "Team Member 2 - T2 - Rahul Ekambaram (rahuleka)\n",
        "| T1 50% | Task 1 | T2 50%  |  \n",
        "| T1 50% | Task 2 | T2 50%  |  \n",
        "| T1 50% | Task 3 | T2 50%  |  \n",
        "| T1 50% | Task 4 | T2 50%  |  \n",
        "| 100% | **Total** |  100% |  \n"
      ],
      "id": "f4cff31e6c6e7e4a"
    },
    {
      "metadata": {
        "id": "4be0a6e29f281e23"
      },
      "cell_type": "code",
      "source": [],
      "id": "4be0a6e29f281e23",
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}